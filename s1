import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD



object wordcount {

  def main(args: Array[String]) {

    val pathToFile = "/home/subhrajit/sparkProjects/data/log.txt"

    

    val conf = new SparkConf()
      .setAppName("Wordcount")
      .setMaster("local[*]")

    val sc = new SparkContext(conf)

    

    val wordsRdd = sc.textFile(pathToFile)
      .flatMap(_.split(" "))

   

    

    val wordCountInitRdd = wordsRdd.map(word => (word, 1))

    

    val wordCountRdd = wordCountInitRdd.reduceByKey((v1, v2) => v1 + v2)
    

    

    val highFreqWords = wordCountRdd.filter(x => x._2 > 4)

    

    highFreqWords.saveAsTextFile("wordcountsDir")
  }
}


name := "sample-spark-project"
version := "1.0"
scalaVersion := "2.10.5"

val sparkVersion = "1.4.1"

// Note the dependencies are provided
libraryDependencies += "org.apache.spark" %% "spark-core" % sparkVersion % "provided"
libraryDependencies += "org.apache.spark" %% "spark-sql" % sparkVersion % "provided"

